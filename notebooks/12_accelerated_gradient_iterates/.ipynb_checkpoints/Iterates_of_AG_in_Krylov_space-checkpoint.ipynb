{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterates of AG in Krylov space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](http://awibisono.github.io/2016/06/20/accelerated-gradient-descent.html), Andre wrote that \"we can unfold the recursion defining accelerated gradient descent and write xk+1 as a linear combination of the previous gradients\", but never really showed it. That is, \n",
    "\n",
    "$$\n",
    "\\boldsymbol x_{k+1} = \\boldsymbol y_0 - \\epsilon \\sum^k_{i=0} \\alpha_{k,i} \\nabla f(\\boldsymbol y_i)\n",
    "$$\n",
    "\n",
    "This is a result often talked about in gradient descent. In the exposition below, I show this and also show that the iterates of the algorithm are in the Krylov space, even though they are not optimal in this space. I couldn't find this calculation anywhere on the internet, so I have decided to put it out there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that we want to minimize the function $f(\\boldsymbol x) = \\frac12 \\boldsymbol x^TH \\boldsymbol x + \\boldsymbol g^T \\boldsymbol x$, where $H$ is positive semidefinite using accelerated gradient descent. With $\\boldsymbol x_0=0$.\n",
    "\n",
    "The two AG iterates $\\boldsymbol x_k$, $\\boldsymbol y_k$ are defined as \n",
    "\n",
    "$$\n",
    "\\boldsymbol x_{k+1} = \\boldsymbol y_k - \\nabla f(\\boldsymbol y_k)/L\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\boldsymbol y_{k+1} = \\boldsymbol x_{k+1} + \\beta_k (\\boldsymbol x_{k+1} - \\boldsymbol x_k)\n",
    "$$\n",
    "\n",
    "When $k=0$, from definition, we have that $\\boldsymbol x_0 = \\boldsymbol y_0 = 0$. In addition, $\\boldsymbol x_1 = \\boldsymbol y_0 - \\nabla f(\\boldsymbol y_0)/L = - \\nabla f(\\boldsymbol y_0)/L$\n",
    "\n",
    "Similarly, $\\boldsymbol y_1 = \\boldsymbol x_1 + \\beta_0 (\\boldsymbol x_1 - \\boldsymbol x_0) = (1+\\beta_0)\\boldsymbol x_1 = -(1 + \\beta_0) \\nabla f(\\boldsymbol y_0)/L = \\alpha_{1,0}\\nabla f(\\boldsymbol y_0)/L$, where $\\alpha_{1,0}= -(1 + \\beta_0)$.\n",
    "\n",
    "When $k=1$, we have that $\\boldsymbol x_2 = \\boldsymbol y_1 - \\nabla f(\\boldsymbol y_1)/L = \\alpha_{1,0}\\nabla f(\\boldsymbol y_0)/L +\\alpha_{1,1}\\nabla f(\\boldsymbol y_1)/L$, where we have substituted in $\\boldsymbol y_1$ and $\\alpha_{1,1}$ is just -1. Similarly, $\\boldsymbol y_2 = \\boldsymbol x_2 + \\beta_1 (\\boldsymbol x_2 - \\boldsymbol x_1) = (1 + \\beta_1)\\boldsymbol x_2 - \\beta_1 \\boldsymbol x_1 = -(1+\\beta_1) \\{ \\alpha_{1,0} \\nabla f(\\boldsymbol y_0)/L + \\alpha_{1,1} \\nabla f(\\boldsymbol y_1)/L  \\} - \\beta_1 \\cdot -\\nabla f(\\boldsymbol y_0)/L = -\\{ (1+\\beta_1)\\alpha_{1,0} - \\beta_1 \\} \\nabla f(\\boldsymbol y_0)/L - (1 + \\beta_1) \\nabla f(\\boldsymbol y_1)/L) =  \\alpha_{2,0} \\nabla f(\\boldsymbol y_0)/L + \\alpha_{2,1} \\nabla f(\\boldsymbol y_1)/L$\n",
    "\n",
    "When $k=2$, we have that $\\boldsymbol x_3 = \\alpha_{2,0} \\nabla f(\\boldsymbol y_0)/L + \\alpha_{2,1} \\nabla f(\\boldsymbol y_1)/L + \\alpha_{2,2} \\nabla f(\\boldsymbol y_2)/L$, etc.\n",
    "\n",
    "In effect, for any $k$, we can write the iterates as linear combinations of $\\nabla f(\\boldsymbol y_i)$, so that \n",
    "\n",
    "$$\n",
    "\\boldsymbol x_{k+1} = \\frac{1}{L}\\sum_{i=0}^k \\alpha_{k,i}\\nabla f(\\boldsymbol y_i)\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\boldsymbol y_{k+1} = \\frac{1}{L}\\sum_{i=0}^k \\alpha_{k+1,i} \\nabla f(\\boldsymbol y_i)\n",
    "$$\n",
    "\n",
    "Now, we have that $\\nabla f(\\boldsymbol y) = H \\boldsymbol y + \\boldsymbol g$, so that $\\nabla f(\\boldsymbol y_0) = \\boldsymbol g$. And also that $\\nabla f(\\boldsymbol y_1) = H \\boldsymbol y_1 + \\boldsymbol g = \\frac{1}{L} \\alpha_{1,0} H \\nabla f(\\boldsymbol y_0) + \\boldsymbol g = \\frac{1}{L}\\alpha_{1,0}H \\boldsymbol g + \\boldsymbol g$.\n",
    "\n",
    "In addition, we can show that $\\nabla f(\\boldsymbol y_2) = \\frac{1}{L^2}\\alpha_{2,1}\\alpha_{1,0}H^2 \\boldsymbol g + \\frac{1}{L}\\alpha_{2,1} H \\boldsymbol g + \\frac{1}{L}\\alpha_{2,0} H \\boldsymbol g + \\boldsymbol g$. In summary, we have that $\\nabla f(\\boldsymbol y_k) = \\sum_{j=1}^k \\gamma_j H^j \\boldsymbol g$, where $\\gamma$ is a constant combination of the $\\alpha$'s.\n",
    "\n",
    "Combining the two equations we have, we can express the iterates as $\\boldsymbol y_{k+1} = \\frac{1}{L} \\sum_{i=0}^k \\alpha_{k+1, i} \\sum_{j=0}^i \\gamma_i H^j \\boldsymbol g$. Similarly, we can write $\\boldsymbol x_{k+1}$ as $\\boldsymbol x_{k+1} = \\frac{1}{L}\\sum_{i=0}^k \\alpha_{k,i} \\sum_{j=0}^i \\gamma_i H^j \\boldsymbol g$.\n",
    "\n",
    "Finally, for any $k$, we can write $\\boldsymbol y_{k+1} = \\alpha_{k+1,0} \\gamma_0 \\boldsymbol g + \\alpha_{k+1, 1} (\\gamma_0 \\boldsymbol g + \\gamma_1 H^1 \\boldsymbol g) + \\alpha_{k+1,2}(\\gamma_0 \\boldsymbol g + \\gamma_1 H^1 \\boldsymbol g + \\gamma_2 H^2 \\boldsymbol g) + \\ldots + \\alpha_{k+1, k}(\\gamma_0 \\boldsymbol g + \\gamma_1 H^1 \\boldsymbol g + \\ldots + \\gamma_k H^k \\boldsymbol g) = \\xi_0 \\boldsymbol g + \\xi_1 H^1 \\boldsymbol g + \\xi_2 H^2 \\boldsymbol g + \\ldots + \\xi_k H^k \\boldsymbol g$, where $\\xi_k$ is a constant combination of the $\\alpha$'s and $\\gamma$'s.\n",
    "\n",
    "We know that if $\\boldsymbol x \\in K_m$ (Krylov space), then $\\boldsymbol x = K_m \\boldsymbol z$ for some $m$ vector $\\boldsymbol z$. Following this, we easily see that $\\boldsymbol y_{k+1} \\in \\text{span}\\{ \\boldsymbol g, H \\boldsymbol g, \\ldots, H^k \\boldsymbol g \\}$. Following the same logic, we find that $\\boldsymbol x_{k+1} \\in \\text{span}\\{ \\boldsymbol g, H \\boldsymbol g, \\ldots, H^k \\boldsymbol g \\}$\n",
    "\n",
    "I implemented Nesterov's accelerated gradient descent [here](https://github.com/manchuran/showables/blob/master/notebooks/12_accelerated_gradient_iterates/nesterov.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
