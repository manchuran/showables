{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of nesterov.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8D_z0IJhnvH"
      },
      "outputs": [],
      "source": [
        "#Implement Nesterov's gradient descent\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (12.0, 9.0)\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Nesterov's gradient descent is provided below. We use the algorithm provided in the class notes, whereby:\n",
        "\n",
        "$\\alpha_0 = 0.5$, $\\:$ $\\alpha^2_{k+1} + (\\alpha_k + 1/\\kappa)\\alpha_{k+1} - \\alpha_k^2$, $\\:$ $\\beta_k = \\frac{\\alpha_k(1 - \\alpha_k)}{\\alpha^2_{k} + \\alpha_{k+1}}$\n",
        "\n",
        "where $\\kappa = \\frac{L}{\\ell}$"
      ],
      "metadata": {
        "id": "_B3oHwR3AbQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov(gradf, L, l, x0, tol, maxit):\n",
        "  y = x0\n",
        "  x = x0\n",
        "  k= 0\n",
        "  alpha = 1/2\n",
        "  kappa_inv = l/L\n",
        "  numit=0\n",
        "  while np.linalg.norm(gradf(y)) > tol and k<maxit:\n",
        "    temp=x\n",
        "    x = y - gradf(y) / L\n",
        "    temp_alpha = alpha #Store \\alpha_k\n",
        "\n",
        "    #Obtain positive root, obtain \\alpha_{k+1}\n",
        "    alpha = (-(alpha**2 - kappa_inv) + np.sqrt((alpha**2 - kappa_inv)**2 + 4*alpha**2)) / 2. \n",
        "    beta = temp_alpha*(1-temp_alpha) / (temp_alpha**2 + alpha)\n",
        "    \n",
        "    y = x + beta *(x - temp)\n",
        "    k+=1\n",
        "    numit+=1\n",
        "  #Check for convergence\n",
        "  if np.linalg.norm(gradf(y)) > tol:\n",
        "    return y, maxit+1\n",
        "\n",
        "  return y, numit"
      ],
      "metadata": {
        "id": "P7SDA5uKhr40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FwaSZSLyZZn_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}